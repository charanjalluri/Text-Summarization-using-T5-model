# -*- coding: utf-8 -*-
"""T5_Supercharged.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E9CfW65FuYlVgCRhIkVhJRnB04MCk-P8
"""

!pip install streamlit pyngrok pillow pydub SpeechRecognition transformers torch sentence-transformers \
langdetect deep-translator PyMuPDF pdfplumber pyvis networkx matplotlib reportlab python-docx gTTS \
google-generativeai faiss-cpu
!pip install spacy && python -m spacy download en_core_web_sm

pip install --upgrade google-genai

import os
os.environ["GEMINI_API_KEY"] = "AIzaSyAfVyAAVZElzPw3mhHBLPsE6VU30WUEgoQ"

!pip install PyPDF2

import os
import io
import tempfile
from typing import List, Dict, Tuple, Optional

import streamlit as st
from PIL import Image
import numpy as np

# PDF handling
try:
    import fitz  # PyMuPDF
    PYMUPDF_OK = True
except Exception:
    PYMUPDF_OK = False

try:
    import pdfplumber
    PDFPLUMBER_OK = True
except Exception:
    PDFPLUMBER_OK = False

from PyPDF2 import PdfReader

# Speech / audio
import speech_recognition as sr
from pydub import AudioSegment

# NLP
import langdetect
from deep_translator import GoogleTranslator

# Transformers (T5)
try:
    import torch
except Exception:
    torch = None
try:
    from transformers import T5Tokenizer, T5ForConditionalGeneration
except Exception:
    T5Tokenizer = None
    T5ForConditionalGeneration = None

# Embeddings / search
try:
    from sentence_transformers import SentenceTransformer
except Exception:
    SentenceTransformer = None

from sklearn.metrics.pairwise import cosine_similarity
try:
    import faiss
    FAISS_OK = True
except Exception:
    FAISS_OK = False

# Graph + viz
import networkx as nx
import matplotlib.pyplot as plt
from pyvis.network import Network
from streamlit.components.v1 import html as st_html

# Exports
from reportlab.pdfgen import canvas as rl_canvas
from docx import Document
from gtts import gTTS

# Gemini new SDK
from google import genai
from google.genai import types

# ------------------------------
# App Config
# ------------------------------
st.set_page_config(
    page_title="ðŸš€ T5/Gemini Summarization Tool (Supercharged)",
    page_icon="ðŸ§ ",
    layout="wide",
)

# ------------------------------
# Load / Configure Gemini
# ------------------------------
GEMINI_DEFAULT_MODEL = "gemini-1.5-flash"
GEMINI_VISION_MODEL = "gemini-1.5-pro-vision"  # Updated model name based on user's finding
GEMINI_EMBED_MODEL = "text-embedding-004"


def ensure_gemini(api_key: Optional[str]):
    if api_key:
        os.environ["GEMINI_API_KEY"] = api_key
    key = os.getenv("GEMINI_API_KEY")
    if not key:
        st.warning("Gemini API key missing. Enter in sidebar.")
        return None
    try:
        return genai.Client(api_key=key)
    except Exception as e:
        st.error(f"Gemini init failed: {e}")
        return None


def get_gemini_client():
    key = os.getenv("GEMINI_API_KEY")
    return genai.Client(api_key=key)


def gemini_summarize(text: str, style="concise", target_lang="en", model=GEMINI_DEFAULT_MODEL) -> str:
    if not text.strip():
        return ""
    client = get_gemini_client()
    try:
        resp = client.models.generate_content(
            model=model,
            contents=[types.Part(text=f"Summarize in {target_lang}, style={style}:\n\n{text}")]
        )
        return resp.text or ""
    except Exception as e:
        st.error(f"Gemini summarization error: {e}")
        return ""


def gemini_qa(question: str, context: str, target_lang="en", model=GEMINI_DEFAULT_MODEL) -> str:
    client = get_gemini_client()
    try:
        resp = client.models.generate_content(
            model=model,
            contents=[
                types.Part(text=f"Context:\n{context}"),
                types.Part(text=f"Question: {question}\nAnswer in {target_lang}")
            ]
        )
        return resp.text or ""
    except Exception as e:
        st.error(f"Gemini Q&A error: {e}")
        return ""


def gemini_vision(uploaded_image, target_lang="en", model=GEMINI_VISION_MODEL) -> str:
    client = get_gemini_client()
    try:
        resp = client.models.generate_content(
            model=model,
            contents=[
                types.Part(
                    inline_data=types.Blob(
                        mime_type=uploaded_image.type,
                        data=uploaded_image.getvalue(),
                    )
                ),
                types.Part(
                    text=f"Describe the content, extract visible text, and summarize. Respond in {target_lang}."
                ),
            ],
        )
        return resp.text or ""
    except Exception as e:
        st.error(f"Gemini vision error: {e}")
        return ""


def gemini_embeddings(texts: List[str]) -> np.ndarray:
    client = get_gemini_client()
    try:
        if isinstance(texts, list):
            res = client.models.embed_content(
                model=GEMINI_EMBED_MODEL,
                contents=[types.Part(text=t) for t in texts],
                config=types.EmbedContentConfig(task_type="RETRIEVAL_DOCUMENT"),
            )
            return np.array([e.values for e in res.embeddings], dtype=np.float32)
        else:
            res = client.models.embed_content(
                model=GEMINI_EMBED_MODEL,
                contents=[types.Part(text=texts)],
                config=types.EmbedContentConfig(task_type="RETRIEVAL_DOCUMENT"),
            )
            return np.array(res.embedding.values, dtype=np.float32)
    except Exception as e:
        st.error(f"Gemini embeddings error: {e}")
        return np.zeros((len(texts), 768), dtype=np.float32)




# ------------------------------
# Cached resources
# ------------------------------
@st.cache_resource(show_spinner=False)
def load_t5(model_name: str):
    if T5Tokenizer is None or T5ForConditionalGeneration is None:
        raise RuntimeError("transformers not available")
    tokenizer = T5Tokenizer.from_pretrained(model_name)
    model = T5ForConditionalGeneration.from_pretrained(model_name)
    device = "cuda" if (torch and torch.cuda.is_available()) else "cpu"
    if device == "cuda":
        model.to("cuda")
    return tokenizer, model, device


@st.cache_resource(show_spinner=False)
def load_embedder():
    if SentenceTransformer is None:
        raise RuntimeError("sentence-transformers not available")
    return SentenceTransformer("all-MiniLM-L6-v2")


@st.cache_resource(show_spinner=False)
def load_spacy():
    try:
        import spacy
        try:
            nlp = spacy.load("en_core_web_sm")
        except OSError:
            from spacy.cli import download
            download("en_core_web_sm")
            nlp = spacy.load("en_core_web_sm")
        return nlp
    except Exception:
        return None


# ------------------------------
# Utilities
# ------------------------------

def detect_language(text: str) -> str:
    try:
        return langdetect.detect(text)
    except Exception:
        return "unknown"


def translate_text(text: str, target_lang: str = "en") -> str:
    if not text:
        return text
    try:
        return GoogleTranslator(source="auto", target=target_lang).translate(text)
    except Exception:
        return text


def chunk_text(text: str, chunk_size: int = 1200, overlap: int = 120) -> List[str]:
    words = text.split()
    chunks = []
    i = 0
    while i < len(words):
        chunk = words[i : i + chunk_size]
        chunks.append(" ".join(chunk))
        i += max(1, chunk_size - overlap)
    return chunks


# ------------------------------
# PDF Extraction
# ------------------------------

def extract_text_from_pdf(file) -> Dict[str, any]:
    """Return {text, pages, tables}
    Uses PyMuPDF if available (best), fallback to pdfplumber, then PyPDF2.
    """
    text_all = []
    tables = []
    pages = 0

    # Buffer the file bytes once for reliable re-opening
    file_bytes = file.read()

    # Try PyMuPDF
    if PYMUPDF_OK:
        try:
            with fitz.open(stream=file_bytes, filetype="pdf") as doc:
                pages = doc.page_count
                for page in doc:
                    text_all.append(page.get_text("text") or "")
        except Exception as e:
            st.info(f"PyMuPDF fell back: {e}")

    # Try pdfplumber
    if PDFPLUMBER_OK:
        try:
            with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:
                if not pages:
                    pages = len(pdf.pages)
                for p in pdf.pages:
                    t = p.extract_text() or ""
                    if t:
                        text_all.append(t)
                    try:
                        tbs = p.extract_tables() or []
                        for tb in tbs:
                            if tb:
                                tables.append(tb)
                    except Exception:
                        pass
        except Exception as e:
            st.info(f"pdfplumber fell back: {e}")

    # Fallback to PyPDF2
    if not text_all:
        try:
            reader = PdfReader(io.BytesIO(file_bytes))
            pages = len(reader.pages)
            for page in reader.pages:
                page_text = page.extract_text() or ""
                text_all.append(page_text)
        except Exception as e:
            return {"text": "", "pages": 0, "tables": [], "error": f"Error reading PDF: {e}"}

    return {"text": "\n".join(text_all), "pages": pages, "tables": tables}


# ------------------------------
# Audio â†’ text
# ------------------------------

def _chunk_audio_to_wav_bytes(uploaded_audio, max_ms: int = 30_000) -> List[bytes]:
    audio = AudioSegment.from_file(io.BytesIO(uploaded_audio.read()))
    chunks = []
    for start in range(0, len(audio), max_ms):
        segment = audio[start : start + max_ms]
        buf = io.BytesIO()
        segment.export(buf, format="wav")
        chunks.append(buf.getvalue())
    return chunks


def transcribe_audio(uploaded_audio, use_whisper: bool = False) -> str:
    # Prefer chunking to avoid Whisper long-form issues
    try:
        wav_chunks = _chunk_audio_to_wav_bytes(uploaded_audio)
    except Exception:
        # fallback: single chunk
        audio = AudioSegment.from_file(io.BytesIO(uploaded_audio.read()))
        buf = io.BytesIO()
        audio.export(buf, format="wav")
        wav_chunks = [buf.getvalue()]

    text_parts: List[str] = []

    if use_whisper:
        try:
            from transformers import pipeline
            asr = pipeline("automatic-speech-recognition", model="openai/whisper-small")
            for wb in wav_chunks:
                with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                    tmp.write(wb)
                    tmp.flush()
                    out = asr(tmp.name, return_timestamps=True)
                text_parts.append(out.get("text", ""))
            return " ".join(text_parts).strip()
        except Exception as e:
            st.error(f"Whisper error, falling back to Google STT: {e}")

    # Fallback: Google STT
    recognizer = sr.Recognizer()
    for wb in wav_chunks:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
            tmp.write(wb)
            tmp.flush()
            with sr.AudioFile(tmp.name) as source:
                audio_data = recognizer.record(source)
                try:
                    text_parts.append(recognizer.recognize_google(audio_data))
                except Exception:
                    pass
    return " ".join(text_parts).strip()


# ------------------------------
# Extractive / Hybrid summaries (local)
# ------------------------------
from sklearn.feature_extraction.text import TfidfVectorizer


def extractive_summary(text: str, max_sentences: int = 3) -> str:
    if not text or not text.strip():
        return ""
    try:
        from nltk.tokenize import sent_tokenize
    except Exception:
        def sent_tokenize(x):
            return [s.strip() for s in x.split('.') if s.strip()]
    sentences = sent_tokenize(text)
    if len(sentences) <= max_sentences:
        return text
    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(sentences)
    scores = X.sum(axis=1).A1
    top_ids = scores.argsort()[-max_sentences:][::-1]
    return " ".join([sentences[i] for i in top_ids])


def load_t5_if_needed(name: str):
    try:
        return load_t5(name)
    except Exception as e:
        st.info(f"T5 model load failed: {e}")
        return None, None, None


def abstractive_t5(text: str, model_name: str, max_length=150, min_length=30, num_beams=4) -> str:
    tok, mdl, dev = load_t5_if_needed(model_name)
    if not tok or not mdl:
        return extractive_summary(text, max_sentences=5)
    input_text = "summarize: " + text
    import torch as _torch
    inputs = tok.encode(input_text, return_tensors="pt", max_length=512, truncation=True)
    if dev == "cuda":
        inputs = inputs.to("cuda")
    out_ids = mdl.generate(
        inputs,
        max_length=max_length,
        min_length=min_length,
        length_penalty=2.0,
        num_beams=num_beams,
        early_stopping=True,
    )
    return tok.decode(out_ids[0], skip_special_tokens=True)


# ------------------------------
# Gemini-powered summarization and Q&A
# ------------------------------

def gemini_summarize(text: str, style: str = "concise", target_lang: str = "en", model_name: str = GEMINI_DEFAULT_MODEL) -> str:
    if not text.strip():
        return ""
    client = get_gemini_client()
    resp = client.models.generate_content(
        model=model_name,
        contents=f"You are a helpful assistant...\nCONTENT:\n{text}"
    )
    return resp.text or ""


def gemini_qa(question: str, context: str, model_name: str = GEMINI_DEFAULT_MODEL, target_lang: str = "en") -> str:
    client = get_gemini_client()
    resp = client.models.generate_content(
        model=model_name,
        contents=f"QUESTION: {question}\n\nCONTEXT:\n{context}"
    )
    return resp.text or ""



from google import genai
from google.genai import types

# Create a reusable Gemini client
def get_gemini_client():
    api_key = os.getenv("GEMINI_API_KEY")
    return genai.Client(api_key=api_key)

def gemini_embeddings(texts: List[str]) -> np.ndarray:
    client = get_gemini_client()
    if isinstance(texts, list):
        res = client.models.embed_content(
            model=GEMINI_EMBED_MODEL,
            contents=[types.Part(text=t) for t in texts],
            config=types.EmbedContentConfig(task_type="RETRIEVAL_DOCUMENT"),
        )
        vecs = np.array([e.values for e in res.embeddings], dtype=np.float32)
    else:
        res = client.models.embed_content(
            model=GEMINI_EMBED_MODEL,
            contents=[types.Part(text=texts)],
            config=types.EmbedContentConfig(task_type="RETRIEVAL_DOCUMENT"),
        )
        vecs = np.array(res.embedding.values, dtype=np.float32)
    return vecs



# ------------------------------
# Semantic search (Gemini + fallback)
# ------------------------------
@st.cache_data(show_spinner=False)
def build_index(chunks: List[str], use_gemini: bool) -> Dict:
    if use_gemini:
        try:
            mat = gemini_embeddings(chunks)
        except Exception as e:
            st.warning(f"Gemini embeddings failed, falling back to MiniLM: {e}")
            use_gemini = False
    if not use_gemini:
        embedder = load_embedder()
        mat = embedder.encode(chunks)
        mat = np.array(mat, dtype=np.float32)

    index = None
    if FAISS_OK:
        index = faiss.IndexFlatIP(mat.shape[1])
        norms = np.linalg.norm(mat, axis=1, keepdims=True) + 1e-12
        mat_norm = mat / norms
        index.add(mat_norm)
        store = {"faiss": True, "index": index, "vecs": mat_norm}
    else:
        store = {"faiss": False, "vecs": mat}
    store["chunks"] = chunks
    store["use_gemini"] = use_gemini
    return store


def search_index(store: Dict, query: str, top_k: int = 3) -> List[Tuple[int, float]]:
    chunks = store["chunks"]
    use_gemini = store.get("use_gemini", False)

    if use_gemini:
        qv = gemini_embeddings([query])[0]
    else:
        embedder = load_embedder()
        qv = embedder.encode([query])[0]
    qv = np.array(qv, dtype=np.float32)

    if store.get("faiss"):
        qv = qv / (np.linalg.norm(qv) + 1e-12)
        D, I = store["index"].search(qv.reshape(1, -1), top_k)
        return list(zip(I[0].tolist(), D[0].tolist()))
    else:
        sims = cosine_similarity([qv], store["vecs"])[0]
        idx = np.argsort(sims)[::-1][:top_k]
        return list(zip(idx.tolist(), sims[idx].tolist()))


# ------------------------------
# Knowledge Graph (NER-based)
# ------------------------------

def build_kg(text: str) -> nx.Graph:
    G = nx.Graph()
    nlp = load_spacy()
    if not nlp:
        tokens = text.split()
        for i in range(len(tokens) - 1):
            a, b = tokens[i], tokens[i + 1]
            G.add_edge(a, b)
        return G
    doc = nlp(text)
    ents = [(e.text, e.label_) for e in doc.ents]
    for ent, label in ents:
        G.add_node(ent, label=label)
    for sent in doc.sents:
        sen_ents = [e.text for e in sent.ents]
        for i in range(len(sen_ents)):
            for j in range(i + 1, len(sen_ents)):
                a, b = sen_ents[i], sen_ents[j]
                if a != b:
                    if G.has_edge(a, b):
                        G[a][b]["weight"] += 1
                    else:
                        G.add_edge(a, b, weight=1)
    return G


def render_pyvis_graph(G: nx.Graph, height: str = "500px"):
    net = Network(height=height, width="100%", bgcolor="#0b1220", font_color="white")
    net.force_atlas_2based()
    for n, data in G.nodes(data=True):
        label = data.get("label", "")
        title = f"{n} ({label})" if label else n
        net.add_node(n, label=n, title=title)
    for u, v, d in G.edges(data=True):
        w = d.get("weight", 1)
        net.add_edge(u, v, value=w)
    with tempfile.NamedTemporaryFile(delete=False, suffix=".html") as tmp:
        net.write_html(tmp.name)
        tmp.flush()
        tmp.seek(0)
        html = tmp.read().decode("utf-8")
    st_html(html, height=height)


# ------------------------------
# Export helpers
# ------------------------------

def save_as_txt(text: str) -> bytes:
    return text.encode("utf-8")


def save_as_pdf(text: str) -> str:
    file_path = "summary.pdf"
    c = rl_canvas.Canvas(file_path)
    c.setFont("Helvetica", 12)
    c.drawString(72, 800, "Summary")
    text_obj = c.beginText(72, 780)
    for line in text.split("\n"):
        for i in range(0, len(line), 95):
            text_obj.textLine(line[i : i + 95])
    c.drawText(text_obj)
    c.save()
    return file_path


def save_as_docx(text: str, title: str = "Summary") -> str:
    doc = Document()
    doc.add_heading(title, level=1)
    for para in text.split("\n\n"):
        doc.add_paragraph(para)
    path = "summary.docx"
    doc.save(path)
    return path


def save_as_md(text: str) -> str:
    path = "summary.md"
    with open(path, "w", encoding="utf-8") as f:
        f.write(text)
    return path


def save_tts(text: str, lang: str = "en") -> str:
    path = "summary.mp3"
    try:
        tts = gTTS(text=text, lang=lang)
        tts.save(path)
        return path
    except Exception as e:
        st.error(f"TTS failed: {e}")
        return ""


# ------------------------------
# UI
# ------------------------------

def sidebar_settings():
    st.sidebar.header("Settings")
    input_type = st.sidebar.selectbox("Select input:", ["Text", "Image", "PDF", "Audio", "Multi-Document"])
    use_gemini_box = st.sidebar.checkbox("Use Gemini (recommended)", value=True)
    gem_key = st.sidebar.text_input("Gemini API Key", type="password")
    client = ensure_gemini(gem_key) if use_gemini_box else None
    t5_name = st.sidebar.selectbox("T5 (fallback/local)", ["t5-small", "t5-base", "google/flan-t5-base"])
    method = st.sidebar.radio("Method", ["Abstractive", "Extractive", "Hybrid"])
    style = st.sidebar.selectbox("Style", ["concise", "bullet-heavy", "academic", "narrative"])
    length_mode = st.sidebar.radio("Length", ["Short", "Medium", "Long"], index=1)
    length_map = {"Short": 80, "Medium": 160, "Long": 320}
    target_lang = st.sidebar.text_input("Target Language Code", "en")
    top_k = st.sidebar.slider("Search top_k", 1, 10, 3)
    show_kg = st.sidebar.checkbox("Show Knowledge Graph", value=False)
    return dict(
        input_type=input_type,
        use_gemini=(use_gemini_box and client is not None),
        t5_name=t5_name,
        method=method,
        style=style,
        length_map=length_map,
        length_mode=length_mode,
        target_lang=target_lang,
        top_k=top_k,
        show_kg=show_kg,
    )


# ------------------------------
# Main
# ------------------------------

def main():
    st.title("T-5 Summarizer â€” Supercharged (^_~)")
    st.write("Text, PDF, Image, Audio, Multi-Doc. Gemini-powered summaries & Q&A, better PDF parsing, entity graphs, and rich exports.")

    cfg = sidebar_settings()

    # History state
    if "history" not in st.session_state:
        st.session_state.history = []

    # Summarization params derived from length
    max_length = cfg["length_map"][cfg["length_mode"]]

    # -------------- Text --------------
    if cfg["input_type"] == "Text":
        user_input = st.text_area("Enter or paste text:", height=240)
        if st.button("Summarize"):
            if not user_input.strip():
                st.warning("Please enter some text.")
            else:
                with st.spinner("Summarizing..."):
                    language = detect_language(user_input)
                    st.write(f"Detected Language: **{language}**")

                    # Use Gemini abstractive by default
                    if cfg["method"] == "Extractive":
                        base = extractive_summary(user_input)
                        final = translate_text(base, cfg["target_lang"]) if cfg["target_lang"] != language else base
                    elif cfg["method"] == "Hybrid":
                        skim = extractive_summary(user_input, max_sentences=6)
                        if cfg["use_gemini"]:
                            base = gemini_summarize(skim, style=cfg["style"], target_lang=cfg["target_lang"])
                        else:
                            base = abstractive_t5(skim, cfg["t5_name"], max_length=max_length)
                            base = translate_text(base, cfg["target_lang"])
                        final = base
                    else:  # Abstractive
                        if cfg["use_gemini"]:
                            final = gemini_summarize(user_input, style=cfg["style"], target_lang=cfg["target_lang"])
                        else:
                            base = abstractive_t5(user_input, cfg["t5_name"], max_length=max_length)
                            final = translate_text(base, cfg["target_lang"]) if cfg["target_lang"] else base

                    st.subheader("Summary")
                    st.write(final)

                    # Downloads
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.download_button("Download TXT", save_as_txt(final), file_name="summary.txt")
                    with col2:
                        pdf_path = save_as_pdf(final)
                        with open(pdf_path, "rb") as f:
                            st.download_button("Download PDF", f, file_name="summary.pdf")
                    with col3:
                        md_path = save_as_md(final)
                        with open(md_path, "rb") as f:
                            st.download_button("Download Markdown", f, file_name="summary.md")
                    with col4:
                        docx_path = save_as_docx(final)
                        with open(docx_path, "rb") as f:
                            st.download_button("Download DOCX", f, file_name="summary.docx")

                    # TTS
                    if st.toggle("Generate audio (TTS)"):
                        mp3 = save_tts(final, lang=cfg["target_lang"] if cfg["target_lang"] else "en")
                        if mp3:
                            audio_file = open(mp3, 'rb')
                            st.audio(audio_file.read(), format='audio/mp3')

    # -------------- Image --------------
    elif cfg["input_type"] == "Image":
        uploaded_image = st.file_uploader("Upload Image", type=["png", "jpg", "jpeg"])
        if uploaded_image and st.button("Extract & Summarize"):
            with st.spinner("Processing image..."):
                image = Image.open(uploaded_image)
                st.image(image, caption="Uploaded image", use_container_width=True)

                extracted = ""
                if cfg["use_gemini"]:
                    try:
                        mclient = get_gemini_client()  # This is the genai.Client returned by ensure_gemini()

                        resp = mclient.models.generate_content(
                            model=GEMINI_VISION_MODEL,
                            contents=[
                                types.Part(
                                    inline_data=types.Blob(
                                        mime_type=uploaded_image.type,
                                        data=uploaded_image.getvalue(),
                                    )
                                ),
                                types.Part(
                                    text=f"Describe the content, then summarize any visible text. Return in language code: {cfg['target_lang']}"
                                ),
                            ],
                        )

                        extracted = resp.text or ""
                    except Exception as e:
                        st.warning(f"Gemini vision failed, using OCR: {e}")
                if not extracted:
                    try:
                        import pytesseract
                        gray = image.convert('L')
                        extracted = pytesseract.image_to_string(gray)
                    except Exception as e:
                        extracted = ""
                        st.error(f"OCR failed: {e}")

                if extracted:
                    st.subheader("Extracted / Described Text")
                    st.write(extracted)
                    # Summarize extracted
                    if cfg["use_gemini"]:
                        final = gemini_summarize(extracted, style=cfg["style"], target_lang=cfg["target_lang"])
                    else:
                        base = abstractive_t5(extracted, cfg["t5_name"], max_length=max_length)
                        final = translate_text(base, cfg["target_lang"]) if cfg["target_lang"] else base
                    st.subheader("Summary")
                    st.write(final)

    # -------------- Audio --------------
    elif cfg["input_type"] == "Audio":
        uploaded_audio = st.file_uploader("Upload Audio", type=["wav", "mp3", "m4a"])
        use_whisper = st.sidebar.checkbox("Use Whisper (better accuracy)")
        if uploaded_audio and st.button("Transcribe & Summarize"):
            with st.spinner("Transcribing audio..."):
                text = transcribe_audio(uploaded_audio, use_whisper)
                st.subheader("Transcript")
                st.write(text if text else "(no speech detected)")
                if text:
                    if cfg["use_gemini"]:
                        final = gemini_summarize(text, style=cfg["style"], target_lang=cfg["target_lang"])
                    else:
                        base = abstractive_t5(text, cfg["t5_name"], max_length=max_length)
                        final = translate_text(base, cfg["target_lang"]) if cfg["target_lang"] else base
                    st.subheader("Summary")
                    st.write(final)

    # -------------- PDF --------------
    elif cfg["input_type"] == "PDF":
        uploaded_pdf = st.file_uploader("Upload PDF", type="pdf")
        if uploaded_pdf:
            with st.spinner("Extracting PDF..."):
                data = extract_text_from_pdf(uploaded_pdf)
            if data.get("error"):
                st.error(data["error"])
                return

            pdf_text = data.get("text", "")
            tables = data.get("tables", [])
            pages = data.get("pages", 0)

            st.success(f"Parsed {pages} pages. Found {len(tables)} table blocks.")
            st.text_area("Raw text (first 2000 chars)", pdf_text[:2000], height=160)

            # Show tables preview
            if tables:
                st.subheader("Extracted Tables (preview)")
                max_show = min(2, len(tables))
                for i in range(max_show):
                    tb = tables[i]
                    st.write(f"Table {i+1} (first 5 rows)")
                    try:
                        import pandas as pd
                        df = pd.DataFrame(tb[1:], columns=tb[0]) if tb and len(tb) > 1 else pd.DataFrame(tb)
                        st.dataframe(df.head())
                    except Exception:
                        st.write(tb[:5])

            # Chunk + index
            chunks = chunk_text(pdf_text, chunk_size=1200, overlap=160)
            if not chunks:
                st.warning("No readable text found in PDF.")
                return

            store = build_index(chunks, use_gemini=cfg["use_gemini"])  # cached

            # PDF actions
            colA, colB = st.columns(2)
            with colA:
                if st.button("Summarize Full PDF"):
                    with st.spinner("Summarizing PDF..."):
                        if cfg["use_gemini"]:
                            prompt = (
                                "Create a hierarchical outline (H1/H2/H3) and a succinct summary of the following document.\n"
                                f"Return in language code: {cfg['target_lang']}.\n"
                                "Include bullet points, key numbers, and named entities.\n"
                                "If tables are present, summarize them too.\n\n"
                            )
                            final = gemini_summarize(prompt + pdf_text, style=cfg["style"], target_lang=cfg["target_lang"])
                        else:
                            parts = []
                            prog = st.progress(0)
                            for i, ch in enumerate(chunks):
                                parts.append(abstractive_t5(ch, cfg["t5_name"], max_length=max_length))
                                prog.progress((i + 1) / len(chunks))
                            base = "\n".join(parts)
                            final = translate_text(base, cfg["target_lang"]) if cfg["target_lang"] else base
                        st.subheader("Full PDF Summary")
                        st.write(final)
                        # Downloads
                        st.download_button("Download TXT", save_as_txt(final), file_name="summary.txt")
                        pdf_path = save_as_pdf(final)
                        with open(pdf_path, "rb") as f:
                            st.download_button("Download PDF", f, file_name="summary.pdf")
                        docx_path = save_as_docx(final, title="PDF Summary")
                        with open(docx_path, "rb") as f:
                            st.download_button("Download DOCX", f, file_name="summary.docx")

            with colB:
                st.markdown("**Ask questions about this PDF**")
                user_q = st.text_input("Your question")
                if st.button("Get Answer") and user_q.strip():
                    hits = search_index(store, user_q, top_k=cfg["top_k"])  # [(idx, score)]
                    context = "\n\n".join([chunks[i] for i, _ in hits])
                    if cfg["use_gemini"]:
                        answer = gemini_qa(user_q, context, target_lang=cfg["target_lang"])
                    else:
                        best_idx = hits[0][0]
                        answer = abstractive_t5(chunks[best_idx], cfg["t5_name"], max_length=max_length)
                        answer = translate_text(answer, cfg["target_lang"]) if cfg["target_lang"] else answer
                    st.write(answer)
                    st.caption("(Answer grounded in top-matching chunks)")
                    st.session_state.history.append((user_q, answer))

            if st.session_state.history:
                with st.expander("Conversation History"):
                    for q, a in st.session_state.history[-20:]:
                        st.markdown(f"**Q:** {q}")
                        st.markdown(f"**A:** {a}")

            if cfg["show_kg"]:
                st.subheader("Knowledge Graph")
                G = build_kg(pdf_text[:15000])  # cap tokens for speed
                render_pyvis_graph(G)

    # -------------- Multi-Document --------------
    elif cfg["input_type"] == "Multi-Document":
        uploaded_files = st.file_uploader("Upload multiple PDFs (and/or .txt)", type=["pdf", "txt"], accept_multiple_files=True)
        if uploaded_files and st.button("Summarize All"):
            with st.spinner("Processing documents..."):
                texts = []
                for f in uploaded_files:
                    if f.type == "application/pdf":
                        data = extract_text_from_pdf(f)
                        texts.append(data.get("text", ""))
                    else:
                        texts.append(f.getvalue().decode("utf-8", errors="ignore"))
                merged = "\n\n".join(texts)
                if cfg["use_gemini"]:
                    final = gemini_summarize(
                        "Create a cross-document summary that compares and contrasts sources. Highlight conflicts, consensus, and key stats.\n" + merged,
                        style=cfg["style"],
                        target_lang=cfg["target_lang"],
                    )
                else:
                    skims = [extractive_summary(t, max_sentences=8) for t in texts]
                    base = abstractive_t5("\n\n".join(skims), cfg["t5_name"], max_length=max_length)
                    final = translate_text(base, cfg["target_lang"]) if cfg["target_lang"] else base
                st.subheader("Multi-Document Summary")
                st.write(final)
                st.download_button("Download TXT", save_as_txt(final), file_name="multi_summary.txt")


if __name__ == "__main__":
    main()

!pip install pyngrok
from pyngrok import ngrok

!ngrok authtoken 2tTQu2ArgMfx2bRNTTINr28VgsW_oaJek2gL18XbzXYyVnPa

!streamlit run app.py &> logs.txt &

from pyngrok import ngrok

# Now, open the tunnel on port 8501
public_url = ngrok.connect(addr="8501")
print("Streamlit App URL:", public_url)

!pip install pytesseract

!pip install google-genai